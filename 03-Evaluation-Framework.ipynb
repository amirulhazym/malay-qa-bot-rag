{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23ce98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the Neutral Evaluation Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "print(\"--- Step 1: Creating a Brand-Neutral Golden Evaluation Dataset ---\")\n",
    "\n",
    "# --- Load the Original, Biased SQuAD Data from V1 ---\n",
    "squad_filepath = 'v1_malay_selfhosted/squad_format_qa_pairs.json'\n",
    "print(f\"Loading original SQuAD data from: {squad_filepath}\")\n",
    "with open(squad_filepath, 'r', encoding='utf-8') as f:\n",
    "    squad_data = json.load(f)\n",
    "\n",
    "# --- Define Neutralization Rules ---\n",
    "# We replace specific brand names with generic e-commerce terms.\n",
    "replacements = {\n",
    "    r'\\bshopee\\b': 'platform',\n",
    "    r'\\blazada\\b': 'platform',\n",
    "    r'\\blazmall\\b': 'premium mall',\n",
    "    r'\\bshopeepay\\b': 'e-wallet',\n",
    "    r'\\blazada wallet\\b': 'e-wallet',\n",
    "    r'\\bshopee coins\\b': 'reward coins',\n",
    "}\n",
    "\n",
    "def neutralize_text(text: str) -> str:\n",
    "    \"\"\"Applies a series of regex replacements to make text brand-agnostic.\"\"\"\n",
    "    for pattern, replacement in replacements.items():\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# --- Flatten and Neutralize the Data ---\n",
    "print(\"Neutralizing questions, answers, and contexts...\")\n",
    "eval_data_neutral = []\n",
    "for article in squad_data['data']:\n",
    "    for paragraph in article['paragraphs']:\n",
    "        neutral_context = neutralize_text(paragraph['context'])\n",
    "        for qa in paragraph['qas']:\n",
    "            if qa['answers']:\n",
    "                neutral_question = neutralize_text(qa['question'])\n",
    "                neutral_answer = neutralize_text(qa['answers'][0]['text'])\n",
    "                \n",
    "                eval_data_neutral.append({\n",
    "                    'question': neutral_question,\n",
    "                    'ground_truth': neutral_answer, # RAGAs expects this key for the true answer\n",
    "                    'contexts': [neutral_context], # RAGAs expects a list of contexts\n",
    "                })\n",
    "\n",
    "# Convert to a Pandas DataFrame\n",
    "eval_df_neutral = pd.DataFrame(eval_data_neutral)\n",
    "\n",
    "# Save the neutralized dataset for review and reusability\n",
    "output_path = \"evaluation_dataset_neutral.csv\"\n",
    "eval_df_neutral.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully created and saved a NEUTRAL evaluation dataset to '{output_path}' with {len(eval_df_neutral)} questions.\")\n",
    "print(\"\\n--- Neutralized Dataset Preview ---\")\n",
    "display(eval_df_neutral.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate Responses from Both V1 and V2 Systems\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "# Add project folders to the Python path to allow importing our adapters\n",
    "sys.path.append('./v1_malay_selfhosted')\n",
    "sys.path.append('./v2_multilingual_api/backend')\n",
    "\n",
    "from v1_adapter import get_v1_rag_response\n",
    "from v2_adapter import get_v2_rag_response\n",
    "\n",
    "print(\"\\n--- Step 2: Generating responses for the evaluation dataset ---\")\n",
    "\n",
    "# Run V2\n",
    "v2_results = []\n",
    "for index, row in tqdm(eval_df_neutral.iterrows(), total=len(eval_df_neutral), desc=\"Evaluating V2 System\"):\n",
    "    response = get_v2_rag_response(row['question'])\n",
    "    v2_results.append({\n",
    "        \"question\": row['question'],\n",
    "        \"answer\": response['answer'],\n",
    "        \"contexts\": response['contexts'],\n",
    "        \"ground_truth\": row['ground_truth']\n",
    "    })\n",
    "    \n",
    "# Run V1\n",
    "v1_results = []\n",
    "for index, row in tqdm(eval_df_neutral.iterrows(), total=len(eval_df_neutral), desc=\"Evaluating V1 System\"):\n",
    "    response = get_v1_rag_response(row['question'])\n",
    "    v1_results.append({\n",
    "        \"question\": row['question'],\n",
    "        \"answer\": response['answer'],\n",
    "        \"contexts\": response['contexts'],\n",
    "        \"ground_truth\": row['ground_truth']\n",
    "    })\n",
    "\n",
    "# Convert results to Hugging Face Dataset objects for RAGAs\n",
    "v1_dataset = Dataset.from_list(v1_results)\n",
    "v2_dataset = Dataset.from_list(v2_results)\n",
    "print(\"\\nResponse generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f10e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Execute RAGAs Evaluation\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
    "\n",
    "# Define the metrics we want to measure\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "]\n",
    "\n",
    "print(\"\\n--- Step 3: Running RAGAs evaluation (this will take time)... ---\")\n",
    "\n",
    "print(\"\\nEvaluating V1...\")\n",
    "v1_scores = evaluate(v1_dataset, metrics)\n",
    "\n",
    "print(\"\\nEvaluating V2...\")\n",
    "v2_scores = evaluate(v2_dataset, metrics)\n",
    "print(\"\\nEvaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2698c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Display and Compare Results\n",
    "import pandas as pd\n",
    "\n",
    "v1_scores_df = v1_scores.to_pandas()\n",
    "v2_scores_df = v2_scores.to_pandas()\n",
    "\n",
    "print(\"--- V1 Evaluation Scores (Detailed) ---\")\n",
    "display(v1_scores_df.head())\n",
    "\n",
    "print(\"\\n--- V2 Evaluation Scores (Detailed) ---\")\n",
    "display(v2_scores_df.head())\n",
    "\n",
    "# Create a final summary comparison table\n",
    "summary_data = {\n",
    "    \"Metric\": [\"Context Precision\", \"Answer Faithfulness\", \"Answer Relevancy\"],\n",
    "    \"V1 Score (Avg)\": [\n",
    "        v1_scores_df['context_precision'].mean(),\n",
    "        v1_scores_df['faithfulness'].mean(),\n",
    "        v1_scores_df['answer_relevancy'].mean()\n",
    "    ],\n",
    "    \"V2 Score (Avg)\": [\n",
    "        v2_scores_df['context_precision'].mean(),\n",
    "        v2_scores_df['faithfulness'].mean(),\n",
    "        v2_scores_df['answer_relevancy'].mean()\n",
    "    ]\n",
    "}\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\\n--- FINAL PERFORMANCE SUMMARY: V1 vs. V2 ---\")\n",
    "display(summary_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
